{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02836223-1004-4a01-b4c0-27b796610069",
   "metadata": {},
   "source": [
    "## Phase 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be46f0-7817-45ee-b95a-0fea19b028a4",
   "metadata": {},
   "source": [
    "### Download and merge both datasets to get all five categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d68c49c-a802-4a10-bd67-522a9b00f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d118c5c9-3ff5-4fa2-81ce-383bc903ec9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://1337x.to/torrent/1048648/American-Snipe...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://1337x.to/torrent/1110018/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://1337x.to/torrent/1122940/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://1337x.to/torrent/1124395/Fast-and-Furio...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://1337x.to/torrent/1145504/Avengers-Age-o...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165361</th>\n",
       "      <td>http://archive.salisburyjournal.co.uk/2001/3/7/</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165362</th>\n",
       "      <td>http://astore.amazon.co.uk/allezvinsfrenchr/de...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165363</th>\n",
       "      <td>http://archive.thisischeshire.co.uk/2000/1/14/...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165364</th>\n",
       "      <td>http://applerugs.co.uk/rugs/product_info.php?p...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165365</th>\n",
       "      <td>http://allgirltogaparty.co.uk/gallery2/main.ph...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165366 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url    type\n",
       "0       http://1337x.to/torrent/1048648/American-Snipe...  benign\n",
       "1       http://1337x.to/torrent/1110018/Blackhat-2015-...  benign\n",
       "2       http://1337x.to/torrent/1122940/Blackhat-2015-...  benign\n",
       "3       http://1337x.to/torrent/1124395/Fast-and-Furio...  benign\n",
       "4       http://1337x.to/torrent/1145504/Avengers-Age-o...  benign\n",
       "...                                                   ...     ...\n",
       "165361    http://archive.salisburyjournal.co.uk/2001/3/7/    spam\n",
       "165362  http://astore.amazon.co.uk/allezvinsfrenchr/de...    spam\n",
       "165363  http://archive.thisischeshire.co.uk/2000/1/14/...    spam\n",
       "165364  http://applerugs.co.uk/rugs/product_info.php?p...    spam\n",
       "165365  http://allgirltogaparty.co.uk/gallery2/main.ph...    spam\n",
       "\n",
       "[165366 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the folder path where your CSV files are stored\n",
    "folder_path = r'./datasets/URL/'\n",
    "\n",
    "# Create a dictionary to map file names (without extension) to types\n",
    "file_to_type = {\n",
    "    'Benign_list_big_final': 'benign',\n",
    "    'DefacementSitesURLFiltered': 'defacement',\n",
    "    'Malware_dataset': 'malware',\n",
    "    'phishing_dataset': 'phishing',\n",
    "    'spam_dataset': 'spam'\n",
    "}\n",
    "\n",
    "# Get the list of CSV files in the folder\n",
    "files_in_folder = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the files in the folder\n",
    "for file in files_in_folder:\n",
    "    # Remove the extension and get the file name without it\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    \n",
    "    # Check if the file name exists in the dictionary\n",
    "    if file_name in file_to_type:\n",
    "        # Read the CSV into a DataFrame (assuming one column with URLs)\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path, header=None)  # No header, as the file only has URLs\n",
    "        \n",
    "        # Rename the column to 'url' since it contains the URLs\n",
    "        df.columns = ['url']\n",
    "        \n",
    "        # Add the 'type' column based on the file name\n",
    "        df['type'] = file_to_type[file_name]\n",
    "        \n",
    "        # Append this DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {file_name} not found in the dictionary.\")\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827898f1-eb73-491b-b916-cbb896ba1c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://1337x.to/torrent/1048648/American-Snipe...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://1337x.to/torrent/1110018/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://1337x.to/torrent/1122940/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://1337x.to/torrent/1124395/Fast-and-Furio...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://1337x.to/torrent/1145504/Avengers-Age-o...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816552</th>\n",
       "      <td>xbox360.ign.com/objects/850/850402.html</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816553</th>\n",
       "      <td>games.teamxbox.com/xbox-360/1860/Dead-Space/</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816554</th>\n",
       "      <td>www.gamespot.com/xbox360/action/deadspace/</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816555</th>\n",
       "      <td>en.wikipedia.org/wiki/Dead_Space_(video_game)</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816556</th>\n",
       "      <td>www.angelfire.com/goth/devilmaycrytonite/</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816557 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url      type\n",
       "0       http://1337x.to/torrent/1048648/American-Snipe...    benign\n",
       "1       http://1337x.to/torrent/1110018/Blackhat-2015-...    benign\n",
       "2       http://1337x.to/torrent/1122940/Blackhat-2015-...    benign\n",
       "3       http://1337x.to/torrent/1124395/Fast-and-Furio...    benign\n",
       "4       http://1337x.to/torrent/1145504/Avengers-Age-o...    benign\n",
       "...                                                   ...       ...\n",
       "816552            xbox360.ign.com/objects/850/850402.html  phishing\n",
       "816553       games.teamxbox.com/xbox-360/1860/Dead-Space/  phishing\n",
       "816554         www.gamespot.com/xbox360/action/deadspace/  phishing\n",
       "816555      en.wikipedia.org/wiki/Dead_Space_(video_game)  phishing\n",
       "816556          www.angelfire.com/goth/devilmaycrytonite/  phishing\n",
       "\n",
       "[816557 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, read the malicious_phish.csv dataset\n",
    "malicious_df = pd.read_csv(r\".\\datasets\\malicious_phish.csv\")\n",
    "\n",
    "# Concatenate malicious_df with final_df (merge both DataFrames)\n",
    "final_df = pd.concat([final_df, malicious_df], ignore_index=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd35d30c-b62b-4a3d-88c3-e6d3c45c0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV\n",
    "final_df.to_csv('./datasets/merged_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a06f69-ef89-424a-8069-112610df59b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://1337x.to/torrent/1048648/American-Snipe...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://1337x.to/torrent/1110018/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://1337x.to/torrent/1122940/Blackhat-2015-...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://1337x.to/torrent/1124395/Fast-and-Furio...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://1337x.to/torrent/1145504/Avengers-Age-o...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816552</th>\n",
       "      <td>xbox360.ign.com/objects/850/850402.html</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816553</th>\n",
       "      <td>games.teamxbox.com/xbox-360/1860/Dead-Space/</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816554</th>\n",
       "      <td>www.gamespot.com/xbox360/action/deadspace/</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816555</th>\n",
       "      <td>en.wikipedia.org/wiki/Dead_Space_(video_game)</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816556</th>\n",
       "      <td>www.angelfire.com/goth/devilmaycrytonite/</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816557 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url      type\n",
       "0       http://1337x.to/torrent/1048648/American-Snipe...    benign\n",
       "1       http://1337x.to/torrent/1110018/Blackhat-2015-...    benign\n",
       "2       http://1337x.to/torrent/1122940/Blackhat-2015-...    benign\n",
       "3       http://1337x.to/torrent/1124395/Fast-and-Furio...    benign\n",
       "4       http://1337x.to/torrent/1145504/Avengers-Age-o...    benign\n",
       "...                                                   ...       ...\n",
       "816552            xbox360.ign.com/objects/850/850402.html  phishing\n",
       "816553       games.teamxbox.com/xbox-360/1860/Dead-Space/  phishing\n",
       "816554         www.gamespot.com/xbox360/action/deadspace/  phishing\n",
       "816555      en.wikipedia.org/wiki/Dead_Space_(video_game)  phishing\n",
       "816556          www.angelfire.com/goth/devilmaycrytonite/  phishing\n",
       "\n",
       "[816557 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.read_csv(r\".\\datasets\\merged_dataset.csv\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3bce3b-eab4-45e1-8cba-bc48419a43bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 80 features\n",
      "Feature Vector: [35, 11, 5, 0, 4, 0, 11, 2, 2, 5.0, 7, 2.0, 0, 28, 7, 3.978864088188099, 3.095795255000934, 1.9219280948873625, 3.0271691184406184, False, 1, 6, 0.14285714285714285, 0.3142857142857143, 0.3142857142857143, 0.45454545454545453, 2.2, 4, 7, 11, False, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def extract_features(url):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    ext = tldextract.extract(url)\n",
    "\n",
    "    domain = ext.domain + \".\" + ext.suffix if ext.suffix else ext.domain\n",
    "    path = parsed_url.path\n",
    "    query = parsed_url.query\n",
    "\n",
    "    # Length-Based Features\n",
    "    url_length = len(url)\n",
    "    domain_length = len(domain)\n",
    "    path_length = len(path)\n",
    "    subdir_length = len(parsed_url.path.rsplit('/', 1)[0]) if '/' in parsed_url.path else 0\n",
    "    filename_length = len(parsed_url.path.split('/')[-1])\n",
    "    file_extension_length = len(parsed_url.path.split('.')[-1]) if '.' in parsed_url.path else 0\n",
    "    arg_length = len(query)\n",
    "\n",
    "    # Token-Based Features\n",
    "    domain_tokens = domain.split('.')\n",
    "    path_tokens = path.split('/')\n",
    "    query_tokens = query.split('&')\n",
    "\n",
    "    avg_domain_token_len = np.mean([len(token) for token in domain_tokens]) if domain_tokens else 0\n",
    "    long_domain_token_len = max([len(token) for token in domain_tokens], default=0)\n",
    "    avg_path_token_len = np.mean([len(token) for token in path_tokens]) if path_tokens else 0\n",
    "\n",
    "    # Character-Based Features\n",
    "    digit_count = sum(c.isdigit() for c in url)\n",
    "    letter_count = sum(c.isalpha() for c in url)\n",
    "    special_char_count = sum(not c.isalnum() for c in url)\n",
    "\n",
    "    # Entropy Features\n",
    "    def calc_entropy(text):\n",
    "        if not text:\n",
    "            return 0\n",
    "        text_count = Counter(text)\n",
    "        probs = [text_count[c] / len(text) for c in text_count]\n",
    "        return entropy(probs, base=2)\n",
    "\n",
    "    url_entropy = calc_entropy(url)\n",
    "    entropy_domain = calc_entropy(domain)\n",
    "    entropy_path = calc_entropy(path)\n",
    "    entropy_query = calc_entropy(query)\n",
    "\n",
    "    # Obfuscation Features\n",
    "    is_ip = bool(re.match(r\"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\", domain))\n",
    "    dot_count = url.count(\".\")\n",
    "    \n",
    "    # Ratio-Based Features\n",
    "    path_url_ratio = path_length / url_length if url_length else 0\n",
    "    arg_url_ratio = arg_length / url_length if url_length else 0\n",
    "    domain_url_ratio = domain_length / url_length if url_length else 0\n",
    "    path_domain_ratio = path_length / domain_length if domain_length else 0\n",
    "    arg_path_ratio = arg_length / path_length if path_length else 0  # FIXED THIS\n",
    "\n",
    "    # Threat Indicators\n",
    "    sensitive_keywords = ['login', 'bank', 'secure', 'account', 'update', 'verify']\n",
    "    url_sensitive_word = any(word in url.lower() for word in sensitive_keywords)\n",
    "\n",
    "    # Executable Indicators\n",
    "    executable = 1 if url.endswith(('.exe', '.zip', '.rar', '.tar', '.gz')) else 0\n",
    "\n",
    "    # Number of symbols\n",
    "    delimiter_count = url.count('.') + url.count('/') + url.count('?') + url.count('=') + url.count('&')\n",
    "\n",
    "    # Max Token Lengths\n",
    "    max_path_token_len = max(len(token) for token in path_tokens) if path_tokens else 0\n",
    "    max_domain_token_len = max(len(token) for token in domain_tokens) if domain_tokens else 0\n",
    "    max_query_token_len = max(len(token) for token in query_tokens) if query_tokens else 0\n",
    "\n",
    "    # Additional Features to Make Exactly 79\n",
    "    number_rate_url = digit_count / url_length if url_length else 0\n",
    "    number_rate_domain = digit_count / domain_length if domain_length else 0\n",
    "    number_rate_path = digit_count / path_length if path_length else 0\n",
    "    number_rate_filename = digit_count / filename_length if filename_length else 0\n",
    "    number_rate_extension = digit_count / file_extension_length if file_extension_length else 0\n",
    "    number_rate_query = digit_count / arg_length if arg_length else 0\n",
    "    \n",
    "    special_domain_count = sum(c in '.-' for c in domain)\n",
    "    special_path_count = sum(c in '.-' for c in path)\n",
    "    special_filename_count = sum(c in '.-' for c in parsed_url.path.split('/')[-1])\n",
    "    special_extension_count = sum(c in '.-' for c in parsed_url.path.split('.')[-1]) if '.' in parsed_url.path else 0\n",
    "    special_query_count = sum(c in '.-' for c in query)\n",
    "\n",
    "    feature_vector = [\n",
    "        url_length, domain_length, path_length, subdir_length, filename_length, file_extension_length, arg_length,\n",
    "        len(domain_tokens), len(path_tokens), avg_domain_token_len, long_domain_token_len, avg_path_token_len,\n",
    "        digit_count, letter_count, special_char_count,\n",
    "        url_entropy, entropy_domain, entropy_path, entropy_query,\n",
    "        is_ip, dot_count, delimiter_count, path_url_ratio, arg_url_ratio, domain_url_ratio, path_domain_ratio, arg_path_ratio,\n",
    "        max_path_token_len, max_domain_token_len, max_query_token_len,\n",
    "        url_sensitive_word, executable,\n",
    "        number_rate_url, number_rate_domain, number_rate_path, number_rate_filename, number_rate_extension, number_rate_query,\n",
    "        special_domain_count, special_path_count, special_filename_count, special_extension_count, special_query_count\n",
    "    ]\n",
    "\n",
    "    # **Ensure Exactly 79 Features**\n",
    "    while len(feature_vector) < 80:\n",
    "        feature_vector.append(0)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# **Test the Function**\n",
    "test_url = \"http://example.com/test?param=value\"\n",
    "features = extract_features(test_url)\n",
    "\n",
    "print(f\"Extracted {len(features)} features\")\n",
    "print(\"Feature Vector:\", features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62a9e6c-bf35-41e7-8c29-a05f9c7f1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 80 features\n",
      "{'urlLen': 35, 'domainlength': 11, 'pathLength': 5, 'subDirLen': 0, 'fileNameLen': 4, 'this.fileExtLen': 0, 'ArgLen': 11, 'domain_token_count': 2, 'path_token_count': 2, 'avgdomaintokenlen': 5.0, 'longdomaintokenlen': 7, 'avgpathtokenlen': 2.0, 'URL_DigitCount': 0, 'URL_Letter_Count': 28, 'spcharUrl': 7, 'Entropy_URL': 3.978864088188099, 'Entropy_Domain': 3.095795255000934, 'Entropy_DirectoryName': 1.9219280948873625, 'Entropy_Afterpath': 3.0271691184406184, 'ISIpAddressInDomainName': False, 'NumberofDotsinURL': 1, 'delimeter_Count': 6, 'pathurlRatio': 0.14285714285714285, 'ArgUrlRatio': 0.3142857142857143, 'domainUrlRatio': 0.3142857142857143, 'pathDomainRatio': 0.45454545454545453, 'argPathRatio': 2.2, 'URL_sensitiveWord': False, 'executable': 0, 'SymbolCount_URL': 1, 'SymbolCount_Domain': 1, 'SymbolCount_Directoryname': 0, 'SymbolCount_Afterpath': 0, 'extra_feature_33': 0, 'extra_feature_34': 0, 'extra_feature_35': 0, 'extra_feature_36': 0, 'extra_feature_37': 0, 'extra_feature_38': 0, 'extra_feature_39': 0, 'extra_feature_40': 0, 'extra_feature_41': 0, 'extra_feature_42': 0, 'extra_feature_43': 0, 'extra_feature_44': 0, 'extra_feature_45': 0, 'extra_feature_46': 0, 'extra_feature_47': 0, 'extra_feature_48': 0, 'extra_feature_49': 0, 'extra_feature_50': 0, 'extra_feature_51': 0, 'extra_feature_52': 0, 'extra_feature_53': 0, 'extra_feature_54': 0, 'extra_feature_55': 0, 'extra_feature_56': 0, 'extra_feature_57': 0, 'extra_feature_58': 0, 'extra_feature_59': 0, 'extra_feature_60': 0, 'extra_feature_61': 0, 'extra_feature_62': 0, 'extra_feature_63': 0, 'extra_feature_64': 0, 'extra_feature_65': 0, 'extra_feature_66': 0, 'extra_feature_67': 0, 'extra_feature_68': 0, 'extra_feature_69': 0, 'extra_feature_70': 0, 'extra_feature_71': 0, 'extra_feature_72': 0, 'extra_feature_73': 0, 'extra_feature_74': 0, 'extra_feature_75': 0, 'extra_feature_76': 0, 'extra_feature_77': 0, 'extra_feature_78': 0, 'extra_feature_79': 0}\n",
      "\n",
      "Missing Features: {'NumberRate_AfterPath', 'dld_filename', 'NumberRate_FileName', 'LongestPathTokenLength', 'SymbolCount_Extension', 'Query_LetterCount', 'sub-Directory_LongestWordLength', 'Extension_DigitCount', 'Directory_DigitCount', 'NumberRate_DirectoryName', 'host_letter_count', 'Directory_LetterCount', 'Querylength', 'ldl_url', 'Entropy_Extension', 'dld_getArg', 'File_name_DigitCount', 'ldl_path', 'tld', 'dld_url', 'CharacterContinuityRate', 'ldl_getArg', 'charcompvowels', 'Extension_LetterCount', 'ldl_filename', 'Path_LongestWordLength', 'NumberRate_Domain', 'charcompace', 'Domain_LongestWordLength', 'dld_path', 'ldl_domain', 'argDomanRatio', 'SymbolCount_FileName', 'delimeter_Domain', 'URLQueries_variable', 'LongestVariableValue', 'Query_DigitCount', 'NumberRate_Extension', 'Filename_LetterCount', 'delimeter_path', 'Arguments_LongestWordLength', 'dld_domain', 'Entropy_Filename', 'isPortEighty', 'host_DigitCount', 'NumberRate_URL'}\n",
      "\n",
      "Extra Features: {'extra_feature_55', 'extra_feature_64', 'extra_feature_40', 'extra_feature_42', 'extra_feature_54', 'extra_feature_77', 'extra_feature_61', 'extra_feature_58', 'extra_feature_74', 'extra_feature_52', 'extra_feature_37', 'extra_feature_67', 'extra_feature_73', 'extra_feature_59', 'extra_feature_57', 'extra_feature_43', 'extra_feature_51', 'extra_feature_36', 'extra_feature_70', 'extra_feature_69', 'extra_feature_49', 'extra_feature_62', 'extra_feature_44', 'extra_feature_72', 'extra_feature_47', 'extra_feature_63', 'extra_feature_66', 'extra_feature_79', 'extra_feature_53', 'extra_feature_68', 'extra_feature_34', 'extra_feature_38', 'extra_feature_71', 'extra_feature_46', 'extra_feature_33', 'extra_feature_39', 'extra_feature_60', 'extra_feature_65', 'extra_feature_76', 'extra_feature_75', 'extra_feature_78', 'extra_feature_50', 'extra_feature_45', 'extra_feature_48', 'extra_feature_41', 'extra_feature_56', 'extra_feature_35'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def extract_features(url):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    ext = tldextract.extract(url)\n",
    "\n",
    "    domain = ext.domain + \".\" + ext.suffix if ext.suffix else ext.domain\n",
    "    path = parsed_url.path\n",
    "    query = parsed_url.query\n",
    "\n",
    "    # Length-Based Features\n",
    "    features = {\n",
    "        'urlLen': len(url),\n",
    "        'domainlength': len(domain),\n",
    "        'pathLength': len(path),\n",
    "        'subDirLen': len(parsed_url.path.rsplit('/', 1)[0]) if '/' in parsed_url.path else 0,\n",
    "        'fileNameLen': len(parsed_url.path.split('/')[-1]),\n",
    "        'this.fileExtLen': len(parsed_url.path.split('.')[-1]) if '.' in parsed_url.path else 0,\n",
    "        'ArgLen': len(query),\n",
    "    }\n",
    "\n",
    "    # Token-Based Features\n",
    "    domain_tokens = domain.split('.')\n",
    "    path_tokens = path.split('/')\n",
    "    query_tokens = query.split('&')\n",
    "\n",
    "    features.update({\n",
    "        'domain_token_count': len(domain_tokens),\n",
    "        'path_token_count': len(path_tokens),\n",
    "        'avgdomaintokenlen': np.mean([len(token) for token in domain_tokens]) if domain_tokens else 0,\n",
    "        'longdomaintokenlen': max([len(token) for token in domain_tokens], default=0),\n",
    "        'avgpathtokenlen': np.mean([len(token) for token in path_tokens]) if path_tokens else 0,\n",
    "    })\n",
    "\n",
    "    # Character-Based Features\n",
    "    features.update({\n",
    "        'URL_DigitCount': sum(c.isdigit() for c in url),\n",
    "        'URL_Letter_Count': sum(c.isalpha() for c in url),\n",
    "        'spcharUrl': sum(not c.isalnum() for c in url),\n",
    "    })\n",
    "\n",
    "    # Entropy Features\n",
    "    def calc_entropy(text):\n",
    "        if not text:\n",
    "            return 0\n",
    "        text_count = Counter(text)\n",
    "        probs = [text_count[c] / len(text) for c in text_count]\n",
    "        return entropy(probs, base=2)\n",
    "\n",
    "    features.update({\n",
    "        'Entropy_URL': calc_entropy(url),\n",
    "        'Entropy_Domain': calc_entropy(domain),\n",
    "        'Entropy_DirectoryName': calc_entropy(path),\n",
    "        'Entropy_Afterpath': calc_entropy(query),\n",
    "    })\n",
    "\n",
    "    # Obfuscation Features\n",
    "    features.update({\n",
    "        'ISIpAddressInDomainName': bool(re.match(r\"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\", domain)),\n",
    "        'NumberofDotsinURL': url.count(\".\"),\n",
    "        'delimeter_Count': url.count('.') + url.count('/') + url.count('?') + url.count('=') + url.count('&'),\n",
    "    })\n",
    "\n",
    "    # Ratio-Based Features\n",
    "    features.update({\n",
    "        'pathurlRatio': features['pathLength'] / features['urlLen'] if features['urlLen'] else 0,\n",
    "        'ArgUrlRatio': features['ArgLen'] / features['urlLen'] if features['urlLen'] else 0,\n",
    "        'domainUrlRatio': features['domainlength'] / features['urlLen'] if features['urlLen'] else 0,\n",
    "        'pathDomainRatio': features['pathLength'] / features['domainlength'] if features['domainlength'] else 0,\n",
    "        'argPathRatio': features['ArgLen'] / features['pathLength'] if features['pathLength'] else 0,\n",
    "    })\n",
    "\n",
    "    # Threat Indicators\n",
    "    sensitive_keywords = ['login', 'bank', 'secure', 'account', 'update', 'verify']\n",
    "    features['URL_sensitiveWord'] = any(word in url.lower() for word in sensitive_keywords)\n",
    "\n",
    "    # Executable Indicators\n",
    "    features['executable'] = 1 if url.endswith(('.exe', '.zip', '.rar', '.tar', '.gz')) else 0\n",
    "\n",
    "    # Symbol Counts\n",
    "    features.update({\n",
    "        'SymbolCount_URL': sum(c in '.-' for c in url),\n",
    "        'SymbolCount_Domain': sum(c in '.-' for c in domain),\n",
    "        'SymbolCount_Directoryname': sum(c in '.-' for c in path),\n",
    "        'SymbolCount_Afterpath': sum(c in '.-' for c in query),\n",
    "    })\n",
    "\n",
    "    # **Ensure Exactly 80 Features**\n",
    "    while len(features) < 80:\n",
    "        features[f\"extra_feature_{len(features)}\"] = 0  # Add dummy features if necessary\n",
    "\n",
    "    return features\n",
    "\n",
    "# **Test the Function**\n",
    "features_dict = extract_features(\"http://example.com/test?param=value\")\n",
    "\n",
    "print(f\"Extracted {len(features_dict)} features\")\n",
    "print(features_dict)\n",
    "# # **Convert to DataFrame**\n",
    "# features_df = pd.DataFrame(merged_df['url'].apply(extract_features).tolist())\n",
    "\n",
    "# # **Check if Column Names Match**\n",
    "# print(\"Matching Columns:\", set(features_df.columns) == set(feature_columns))\n",
    "\n",
    "# # **Save the Dataset**\n",
    "# final_df = pd.concat([merged_df[['url']], features_df, merged_df[['type']]], axis=1)\n",
    "# final_df.to_csv(\"./datasets/final_dataset.csv\", index=False)\n",
    "\n",
    "# print(final_df.head())\n",
    "\n",
    "# Convert extracted features to a set\n",
    "extracted_feature_names = set(features_dict.keys())\n",
    "\n",
    "# Convert expected feature names to a set\n",
    "expected_feature_names = set(feature_columns)\n",
    "\n",
    "# Find missing and extra features\n",
    "missing_features = expected_feature_names - extracted_feature_names\n",
    "extra_features = extracted_feature_names - expected_feature_names\n",
    "\n",
    "print(\"\\nMissing Features:\", missing_features)\n",
    "print(\"\\nExtra Features:\", extra_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ce9b5c3-110d-48a1-b35e-f08ccaeb7d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 79 features\n",
      "{'Querylength': 11, 'domain_token_count': 2, 'path_token_count': 2, 'avgdomaintokenlen': 5.0, 'longdomaintokenlen': 7, 'avgpathtokenlen': 2.0, 'tld': 'com', 'charcompvowels': 10, 'charcompace': 0, 'ldl_url': 35, 'ldl_domain': 11, 'ldl_path': 5, 'ldl_filename': 4, 'ldl_getArg': 11, 'dld_url': 24, 'dld_domain': 8, 'dld_path': -6, 'dld_filename': 0, 'dld_getArg': 11, 'urlLen': 35, 'domainlength': 11, 'pathLength': 5, 'subDirLen': 0, 'fileNameLen': 4, 'this.fileExtLen': 0, 'ArgLen': 11, 'pathurlRatio': 0.14285714285714285, 'ArgUrlRatio': 0.3142857142857143, 'argDomanRatio': 1.0, 'domainUrlRatio': 0.3142857142857143, 'pathDomainRatio': 0.45454545454545453, 'argPathRatio': 2.2, 'executable': 0, 'isPortEighty': False, 'NumberofDotsinURL': 1, 'ISIpAddressInDomainName': False, 'CharacterContinuityRate': 0.05714285714285714, 'LongestVariableValue': 11, 'URL_DigitCount': 0, 'host_DigitCount': 0, 'Directory_DigitCount': 0, 'File_name_DigitCount': 0, 'Extension_DigitCount': 0, 'Query_DigitCount': 0, 'URL_Letter_Count': 28, 'host_letter_count': 10, 'Directory_LetterCount': 4, 'Filename_LetterCount': 4, 'Extension_LetterCount': 3, 'Query_LetterCount': 10, 'LongestPathTokenLength': 4, 'Domain_LongestWordLength': 7, 'Path_LongestWordLength': 4, 'sub-Directory_LongestWordLength': 0, 'Arguments_LongestWordLength': 11, 'URL_sensitiveWord': False, 'URLQueries_variable': 1, 'spcharUrl': 7, 'delimeter_Domain': 0, 'delimeter_path': 0, 'delimeter_Count': 6, 'NumberRate_URL': 0.0, 'NumberRate_Domain': 0.0, 'NumberRate_DirectoryName': 0.0, 'NumberRate_FileName': 0.0, 'NumberRate_Extension': 0.0, 'NumberRate_AfterPath': 0.0, 'SymbolCount_URL': 0, 'SymbolCount_Domain': 0, 'SymbolCount_Directoryname': 0, 'SymbolCount_FileName': 0, 'SymbolCount_Extension': 0, 'SymbolCount_Afterpath': 0, 'Entropy_URL': 3.978864088188099, 'Entropy_Domain': 3.095795255000934, 'Entropy_DirectoryName': 1.9219280948873625, 'Entropy_Filename': 1.5, 'Entropy_Extension': 1.584962500721156, 'Entropy_Afterpath': 3.0271691184406184}\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calc_entropy(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    text_count = Counter(text)\n",
    "    probs = [text_count[c] / len(text) for c in text_count]\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "def extract_features(url):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    ext = tldextract.extract(url)\n",
    "\n",
    "    domain = ext.domain + \".\" + ext.suffix if ext.suffix else ext.domain\n",
    "    path = parsed_url.path\n",
    "    query = parsed_url.query\n",
    "    tld = ext.suffix\n",
    "\n",
    "    domain_tokens = domain.split('.')\n",
    "    path_tokens = path.split('/')\n",
    "    query_tokens = query.split('&')\n",
    "    \n",
    "    features = {\n",
    "        'Querylength': len(query),\n",
    "        'domain_token_count': len(domain_tokens),\n",
    "        'path_token_count': len(path_tokens),\n",
    "        'avgdomaintokenlen': np.mean([len(token) for token in domain_tokens]) if domain_tokens else 0,\n",
    "        'longdomaintokenlen': max([len(token) for token in domain_tokens], default=0),\n",
    "        'avgpathtokenlen': np.mean([len(token) for token in path_tokens]) if path_tokens else 0,\n",
    "        'tld': tld,\n",
    "        'charcompvowels': sum(c in 'aeiou' for c in url),\n",
    "        'charcompace': sum(c.isspace() for c in url),\n",
    "        'ldl_url': len(url),\n",
    "        'ldl_domain': len(domain),\n",
    "        'ldl_path': len(path),\n",
    "        'ldl_filename': len(path.split('/')[-1]),\n",
    "        'ldl_getArg': len(query),\n",
    "        'dld_url': len(url) - len(domain),\n",
    "        'dld_domain': len(domain) - len(tld),\n",
    "        'dld_path': len(path) - len(domain),\n",
    "        'dld_filename': len(path.split('/')[-1]) if '.' in path else 0,\n",
    "        'dld_getArg': len(query),\n",
    "        'urlLen': len(url),\n",
    "        'domainlength': len(domain),\n",
    "        'pathLength': len(path),\n",
    "        'subDirLen': len(parsed_url.path.rsplit('/', 1)[0]) if '/' in parsed_url.path else 0,\n",
    "        'fileNameLen': len(parsed_url.path.split('/')[-1]),\n",
    "        'this.fileExtLen': len(parsed_url.path.split('.')[-1]) if '.' in parsed_url.path else 0,\n",
    "        'ArgLen': len(query),\n",
    "        'pathurlRatio': len(path) / len(url) if len(url) else 0,\n",
    "        'ArgUrlRatio': len(query) / len(url) if len(url) else 0,\n",
    "        'argDomanRatio': len(query) / len(domain) if len(domain) else 0,\n",
    "        'domainUrlRatio': len(domain) / len(url) if len(url) else 0,\n",
    "        'pathDomainRatio': len(path) / len(domain) if len(domain) else 0,\n",
    "        'argPathRatio': len(query) / len(path) if len(path) else 0,\n",
    "        'executable': 1 if url.endswith(('.exe', '.zip', '.rar', '.tar', '.gz')) else 0,\n",
    "        'isPortEighty': ':80' in url,\n",
    "        'NumberofDotsinURL': url.count('.'),\n",
    "        'ISIpAddressInDomainName': bool(re.match(r\"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\", domain)),\n",
    "        'CharacterContinuityRate': sum(1 for i in range(len(url)-1) if url[i] == url[i+1]) / len(url) if len(url) > 1 else 0,\n",
    "        'LongestVariableValue': max(map(len, query_tokens), default=0),\n",
    "        'URL_DigitCount': sum(c.isdigit() for c in url),\n",
    "        'host_DigitCount': sum(c.isdigit() for c in domain),\n",
    "        'Directory_DigitCount': sum(c.isdigit() for c in path),\n",
    "        'File_name_DigitCount': sum(c.isdigit() for c in path.split('/')[-1]),\n",
    "        'Extension_DigitCount': sum(c.isdigit() for c in tld),\n",
    "        'Query_DigitCount': sum(c.isdigit() for c in query),\n",
    "        'URL_Letter_Count': sum(c.isalpha() for c in url),\n",
    "        'host_letter_count': sum(c.isalpha() for c in domain),\n",
    "        'Directory_LetterCount': sum(c.isalpha() for c in path),\n",
    "        'Filename_LetterCount': sum(c.isalpha() for c in path.split('/')[-1]),\n",
    "        'Extension_LetterCount': sum(c.isalpha() for c in tld),\n",
    "        'Query_LetterCount': sum(c.isalpha() for c in query),\n",
    "        'LongestPathTokenLength': max(map(len, path_tokens), default=0),\n",
    "        'Domain_LongestWordLength': max(map(len, domain_tokens), default=0),\n",
    "        'Path_LongestWordLength': max(map(len, path_tokens), default=0),\n",
    "        'sub-Directory_LongestWordLength': max(map(len, path_tokens[:-1]), default=0) if path_tokens else 0,\n",
    "        'Arguments_LongestWordLength': max(map(len, query_tokens), default=0),\n",
    "        'URL_sensitiveWord': any(word in url.lower() for word in ['login', 'bank', 'secure', 'account', 'update', 'verify']),\n",
    "        'URLQueries_variable': len(query_tokens),\n",
    "        'spcharUrl': sum(not c.isalnum() for c in url),\n",
    "        'delimeter_Domain': domain.count('-'),\n",
    "        'delimeter_path': path.count('-'),\n",
    "        'delimeter_Count': url.count('.') + url.count('/') + url.count('?') + url.count('=') + url.count('&'),\n",
    "        'NumberRate_URL': sum(c.isdigit() for c in url) / len(url) if len(url) else 0,\n",
    "        'NumberRate_Domain': sum(c.isdigit() for c in domain) / len(domain) if len(domain) else 0,\n",
    "        'NumberRate_DirectoryName': sum(c.isdigit() for c in path) / len(path) if len(path) else 0,\n",
    "        'NumberRate_FileName': sum(c.isdigit() for c in path.split('/')[-1]) / len(path.split('/')[-1]) if len(path.split('/')[-1]) else 0,\n",
    "        'NumberRate_Extension': sum(c.isdigit() for c in tld) / len(tld) if len(tld) else 0,\n",
    "        'NumberRate_AfterPath': sum(c.isdigit() for c in query) / len(query) if len(query) else 0,\n",
    "        'SymbolCount_URL': url.count('-'),\n",
    "        'SymbolCount_Domain': domain.count('-'),\n",
    "        'SymbolCount_Directoryname': path.count('-'),\n",
    "        'SymbolCount_FileName': path.split('/')[-1].count('-'),\n",
    "        'SymbolCount_Extension': tld.count('-'),\n",
    "        'SymbolCount_Afterpath': query.count('-'),\n",
    "        'Entropy_URL': calc_entropy(url),\n",
    "        'Entropy_Domain': calc_entropy(domain),\n",
    "        'Entropy_DirectoryName': calc_entropy(path),\n",
    "        'Entropy_Filename': calc_entropy(path.split('/')[-1]),\n",
    "        'Entropy_Extension': calc_entropy(tld),\n",
    "        'Entropy_Afterpath': calc_entropy(query)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# **Test the Function**\n",
    "features_dict = extract_features(\"http://example.com/test?param=value\")\n",
    "\n",
    "print(f\"Extracted {len(features_dict)} features\")\n",
    "print(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0660e186-2237-40ad-8e5f-81169837b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  Querylength  \\\n",
      "0  http://1337x.to/torrent/1048648/American-Snipe...            0   \n",
      "1  http://1337x.to/torrent/1110018/Blackhat-2015-...            0   \n",
      "2  http://1337x.to/torrent/1122940/Blackhat-2015-...            0   \n",
      "3  http://1337x.to/torrent/1124395/Fast-and-Furio...            0   \n",
      "4  http://1337x.to/torrent/1145504/Avengers-Age-o...            0   \n",
      "\n",
      "   domain_token_count  path_token_count  avgdomaintokenlen  \\\n",
      "0                   2                 5                3.5   \n",
      "1                   2                 5                3.5   \n",
      "2                   2                 5                3.5   \n",
      "3                   2                 5                3.5   \n",
      "4                   2                 5                3.5   \n",
      "\n",
      "   longdomaintokenlen  avgpathtokenlen tld  charcompvowels  charcompace  ...  \\\n",
      "0                   5             12.8  to              10            0  ...   \n",
      "1                   5             12.8  to               5            0  ...   \n",
      "2                   5             12.8  to               8            0  ...   \n",
      "3                   5             12.8  to              11            0  ...   \n",
      "4                   5             12.8  to              12            0  ...   \n",
      "\n",
      "   SymbolCount_FileName  SymbolCount_Extension  SymbolCount_Afterpath  \\\n",
      "0                     0                      0                      0   \n",
      "1                     0                      0                      0   \n",
      "2                     0                      0                      0   \n",
      "3                     0                      0                      0   \n",
      "4                     0                      0                      0   \n",
      "\n",
      "   Entropy_URL  Entropy_Domain  Entropy_DirectoryName  Entropy_Filename  \\\n",
      "0     4.876201            2.75               4.638502               0.0   \n",
      "1     4.920700            2.75               4.788878               0.0   \n",
      "2     4.821630            2.75               4.671231               0.0   \n",
      "3     4.889040            2.75               4.748150               0.0   \n",
      "4     4.772234            2.75               4.559325               0.0   \n",
      "\n",
      "   Entropy_Extension  Entropy_Afterpath    type  \n",
      "0                1.0                0.0  benign  \n",
      "1                1.0                0.0  benign  \n",
      "2                1.0                0.0  benign  \n",
      "3                1.0                0.0  benign  \n",
      "4                1.0                0.0  benign  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction function to all URLs and create a DataFrame\n",
    "features_df = merged_df['url'].apply(extract_features).apply(pd.Series)\n",
    "\n",
    "# Ensure the feature column names match the expected feature_columns list\n",
    "features_df = features_df[feature_columns]\n",
    "\n",
    "# Concatenate 'url', extracted features, and 'type' in order\n",
    "final_df = pd.concat([merged_df[['url']], features_df, merged_df[['type']]], axis=1)\n",
    "\n",
    "# Save the updated dataset\n",
    "final_df.to_csv(\"./datasets/Final_dataset.csv\", index=False)\n",
    "\n",
    "# Print sample output\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa19147a-9946-4999-b8f7-2d3dba10fe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>Querylength</th>\n",
       "      <th>domain_token_count</th>\n",
       "      <th>path_token_count</th>\n",
       "      <th>avgdomaintokenlen</th>\n",
       "      <th>longdomaintokenlen</th>\n",
       "      <th>avgpathtokenlen</th>\n",
       "      <th>tld</th>\n",
       "      <th>charcompvowels</th>\n",
       "      <th>charcompace</th>\n",
       "      <th>...</th>\n",
       "      <th>SymbolCount_FileName</th>\n",
       "      <th>SymbolCount_Extension</th>\n",
       "      <th>SymbolCount_Afterpath</th>\n",
       "      <th>Entropy_URL</th>\n",
       "      <th>Entropy_Domain</th>\n",
       "      <th>Entropy_DirectoryName</th>\n",
       "      <th>Entropy_Filename</th>\n",
       "      <th>Entropy_Extension</th>\n",
       "      <th>Entropy_Afterpath</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>816552</th>\n",
       "      <td>xbox360.ign.com/objects/850/850402.html</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>com</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.355539</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>4.355539</td>\n",
       "      <td>3.277613</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816553</th>\n",
       "      <td>games.teamxbox.com/xbox-360/1860/Dead-Space/</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>8</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>com</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.243300</td>\n",
       "      <td>3.084963</td>\n",
       "      <td>4.243300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816554</th>\n",
       "      <td>www.gamespot.com/xbox360/action/deadspace/</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>8</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>com</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.147921</td>\n",
       "      <td>3.251629</td>\n",
       "      <td>4.147921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816555</th>\n",
       "      <td>en.wikipedia.org/wiki/Dead_Space_(video_game)</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>org</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.102313</td>\n",
       "      <td>3.334679</td>\n",
       "      <td>4.102313</td>\n",
       "      <td>3.675311</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816556</th>\n",
       "      <td>www.angelfire.com/goth/devilmaycrytonite/</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>com</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.143541</td>\n",
       "      <td>3.546594</td>\n",
       "      <td>4.143541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  Querylength  \\\n",
       "816552        xbox360.ign.com/objects/850/850402.html            0   \n",
       "816553   games.teamxbox.com/xbox-360/1860/Dead-Space/            0   \n",
       "816554     www.gamespot.com/xbox360/action/deadspace/            0   \n",
       "816555  en.wikipedia.org/wiki/Dead_Space_(video_game)            0   \n",
       "816556      www.angelfire.com/goth/devilmaycrytonite/            0   \n",
       "\n",
       "        domain_token_count  path_token_count  avgdomaintokenlen  \\\n",
       "816552                   2                 4                3.0   \n",
       "816553                   2                 5                5.5   \n",
       "816554                   2                 5                5.5   \n",
       "816555                   2                 3                6.0   \n",
       "816556                   2                 4                6.0   \n",
       "\n",
       "        longdomaintokenlen  avgpathtokenlen  tld  charcompvowels  charcompace  \\\n",
       "816552                   3         9.000000  com               5            0   \n",
       "816553                   8         8.000000  com              11            0   \n",
       "816554                   8         7.600000  com              12            0   \n",
       "816555                   9        14.333333  org              18            0   \n",
       "816556                   9         9.500000  com              12            0   \n",
       "\n",
       "        ...  SymbolCount_FileName  SymbolCount_Extension  \\\n",
       "816552  ...                     0                      0   \n",
       "816553  ...                     0                      0   \n",
       "816554  ...                     0                      0   \n",
       "816555  ...                     0                      0   \n",
       "816556  ...                     0                      0   \n",
       "\n",
       "        SymbolCount_Afterpath  Entropy_URL  Entropy_Domain  \\\n",
       "816552                      0     4.355539        2.807355   \n",
       "816553                      0     4.243300        3.084963   \n",
       "816554                      0     4.147921        3.251629   \n",
       "816555                      0     4.102313        3.334679   \n",
       "816556                      0     4.143541        3.546594   \n",
       "\n",
       "        Entropy_DirectoryName  Entropy_Filename  Entropy_Extension  \\\n",
       "816552               4.355539          3.277613           1.584963   \n",
       "816553               4.243300          0.000000           1.584963   \n",
       "816554               4.147921          0.000000           1.584963   \n",
       "816555               4.102313          3.675311           1.584963   \n",
       "816556               4.143541          0.000000           1.584963   \n",
       "\n",
       "        Entropy_Afterpath      type  \n",
       "816552                0.0  phishing  \n",
       "816553                0.0  phishing  \n",
       "816554                0.0  phishing  \n",
       "816555                0.0  phishing  \n",
       "816556                0.0  phishing  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b918935-10e7-4d57-a794-b42f093a19b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                      http://likemag.com/sites/default/files/css/css...\n",
       "Querylength                                                              0\n",
       "domain_token_count                                                       2\n",
       "path_token_count                                                         6\n",
       "avgdomaintokenlen                                                      5.0\n",
       "                                               ...                        \n",
       "Entropy_DirectoryName                                              4.87505\n",
       "Entropy_Filename                                                  4.778119\n",
       "Entropy_Extension                                                 1.584963\n",
       "Entropy_Afterpath                                                      0.0\n",
       "type                                                                benign\n",
       "Name: 12002, Length: 81, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[12002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7ad18-175c-41b9-9d60-d2a3d08d42a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9392cbcf-5fec-4ce7-a394-75f87d599834",
   "metadata": {},
   "source": [
    "### Handle missing values, duplicates, and inconsistencies.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfb2f3-baa1-4a51-90f4-e62ba953891a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79978730-5ebe-481c-b6b2-4c06cf2ee7f6",
   "metadata": {},
   "source": [
    "### Balance classes using SMOTE, oversampling, or undersampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc1e20-c851-4a24-aeac-f916d3eba589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f4c2102-b40b-446f-98dd-f8fa94b7ad6b",
   "metadata": {},
   "source": [
    "## Phase 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafef75-6447-4a43-be53-ad7476a426f3",
   "metadata": {},
   "source": [
    "### Visualize URL distributions, token lengths, and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b94264-8e68-45c6-8749-1166c627d76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69db4586-a571-4b9e-b489-9d56119a8b59",
   "metadata": {},
   "source": [
    "### Generate at least five meaningful graphs to highlight patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48627abd-05a2-4246-83c5-d26710e8e826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d22e589-6b85-492b-9ae3-dff220400106",
   "metadata": {},
   "source": [
    "### Identify relationships between URL structure and malicious behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3e118-a073-4c65-8edc-e6a9fe51399e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5fae57c-d19b-414b-9f76-a62551107518",
   "metadata": {},
   "source": [
    "## Phase 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55415245-fe1a-4c66-ba18-f345827129d5",
   "metadata": {},
   "source": [
    "### Extract structural features (length, subdomains, special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab4880-c7c0-4b9d-96b6-14b0d3d07fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08666a19-eacc-4d43-ad97-c4bfd1cdabfd",
   "metadata": {},
   "source": [
    "### Use TF-IDF, Word2Vec, or Transformers for NLP-based embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf8ce3-9310-42e8-8671-08bad1792975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05da7a35-968c-4b41-9614-e611eca3b8a0",
   "metadata": {},
   "source": [
    "### Consider character-level models for sequence-based feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354cf00-c3c2-4a2a-b119-edf171638ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d33f72-9f39-4e23-b8e6-560d40133d15",
   "metadata": {},
   "source": [
    "## Phase 4: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5aab15-cceb-4e47-94d1-03615d71cf43",
   "metadata": {},
   "source": [
    "### Train 3 Traditional ML models (Random Forest, SVM, XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493182cb-d1de-47f1-916d-9caa70118e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe9dba2-8f07-416d-b5d2-5aae62222304",
   "metadata": {},
   "source": [
    "### Train Deep Learning models (LSTM, CNN) for sequence-based classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd11a0-9151-4365-b917-2a4b1729e16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6fc7e6b-06ff-4a57-83e3-5d00d259b701",
   "metadata": {},
   "source": [
    "### Fine-tune BERT/GPT to capture deeper URL semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03562fdb-ef1c-4b45-9603-a60c12b2a053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb81ef5e-fd3e-484d-99d6-a96f15e308f2",
   "metadata": {},
   "source": [
    "### Compare model performance with confusion matrices & ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39577319-2565-465d-9d7b-fc643527a5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308e6f9-b5f0-4683-8f21-e900fec46af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad3dc4-cfb5-40a5-8b6a-1065b8730bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
